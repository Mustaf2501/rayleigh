{"name":"Rayleigh","tagline":"","body":"# Rayleigh: search images by multiple colors\r\n\r\nWe present an open-source system for quickly searching large image collections by multiple colors given as a palette, or by color similarity to a query image.\r\nThe system is running at [multicolorsearch.com](http://multicolorsearch.com).\r\n\r\n### In brief\r\n\r\n- Images are represented as histograms over a fixed palette of colors in CIELab space.\r\n- Sets of colors used for searching are similarly represented as histograms.\r\n- Similarity of images to other images and to a query set of colors is formulated in terms of distance between histograms.\r\n- To make matching more robust to small differences in color, histograms are smoothed.\r\n\r\n### System components\r\n\r\n- The back end processes image URLs to extract color information to store in a searchable database.\r\n- The web server provides REST access to the back end.\r\n- The client provides the UI for selecting a color palette to search with, or an image, and requests data from the web server.\r\n\r\n### Documentation\r\n\r\nIn addition to this high-level presentation, code is documented at [Rayleigh ReadTheDocs](https://rayleigh.readthedocs.org/en/latest/).\r\n\r\nA public [Trello](https://trello.com/board/rayleigh/50d36a9e0f87f42952000276) contains current and archived tasks.\r\n\r\n### Screenshots\r\n\r\n<img src=\"doc/images/black_yellow.png\" width=\"640px\" /><br />\r\nRayleigh in the search-by-palette view.\r\n\r\n---\r\n\r\n<img src=\"doc/images/image_similarity.png\" width=\"640px\" /><br />\r\nRayleigh in the search-by-image view.\r\n\r\n## Prior Art\r\n\r\n<!--\r\n    I'm having a difficult time understanding how the work you are detailing below will build on CBIR, Idee Labs Multicolr Search, or any of your other examples. Also, can you explain in an additional sentence how later research has closed the \"semantic gap\" in terms of nouns and verbs? Might be a lot to do in a post like this but I'd sort of like a state of the field description earlier on.\r\n\r\n    Jessica\r\n-->\r\n\r\nThe Content-Based Image Retrieval (CBIR) field has done much work on image similarity in the past couple of decades.\r\nMy brief impression of the field is that color similarity was initially seen as proxy for general visual similarity, not an end to itself.\r\nMore recent research focuses on closing the \"semantic gap\" in terms of nouns and verbs.\r\n\r\nI was able to find two implementations of multi-color searching, both closed-source:\r\n\r\n- the excellent [Idee Labs Multicolr Search](http://labs.tineye.com/multicolr), a rather expensive commercial service;\r\n- an experiment called [Chromatik](http://chromatik.labs.exalead.com/).\r\n\r\nThere is an open-source project on top of Apache Lucene for general image retrieval, called [Lire](http://www.semanticmetadata.net/wiki/doku.php?id=lire:lire).\r\nI have not tried it.\r\n\r\nAdditionally, a simple open-source implementation of a [single-color search](http://99designs.com/tech-blog/blog/2012/08/02/color-explorer/) was useful to see.\r\n\r\n## Perception of Color\r\n\r\nAll visual perception is highly complex.\r\nAs countless optical illusions show,\r\n\r\n- we perceive objects of the same actual size in the image as being [different sizes](http://www.illusionspoint.com/wp-content/uploads/2010/09/relative-size-optical-illusion-21.jpg);\r\n- we see lines where there [aren't any](http://i.telegraph.co.uk/multimedia/archive/01121/300px-kanizsa-tria_1121094i.jpg);\r\n- and most relevantly for this project, we see things that are the same color in the image as being [different colors](http://brainden.com/color-illusions.htm).\r\n\r\nTo get a machine to \"see\" the illusory contour of the triangle in the second example is an unsolved problem in practice.\r\nIf we were developing a system to search images by shape, we would struggle to return the Kanizsa Triangle above as a result for \"triangle.\"\r\n\r\nSo it is with color.\r\n\r\n- As artists have [long understood](http://www.amazon.com/Interaction-Color-Revised-Josef-Albers/dp/0300018460), how we see a color depends on what other colors are around it.\r\n\r\n- As in the linked illusions above, the same color values in an image can be perceived as two different colors.\r\n\r\n- Or conversely, we may perceive an object as being the same color in different images where its actual color is different (for example, you perceive your face to be the same color in an outdoors photograph, bathroom mirror, fluorescent office, and in the darkness of evening).\r\n\r\n- On top of all this, there is a layer of language on top of all this mess:\r\ndifferent languages deliniate colors in slightly different ways (for example, Russian has two distinct colors of \"blue\"), and it has been shown that it actually [affects color perception](http://boingboing.net/2011/08/12/how-language-affects-color-perception.html).\r\n    \r\n## Representing Color\r\n\r\nSo the problem of searching images by color is hard, but we will still attempt it.\r\nHow should we represent \"color\" in the search engine?\r\n\r\n### Human eye\r\n\r\nIn our eyes, there are two types of photorceptive cells: rods and cones.\r\nRods respond only to intensity of light, not its color, and are far more numerous.\r\nCones have three distinct types ('S', 'M', 'L'), each responding strongest to a specific wavelength of light.\r\nOur perception of color is derived from the response rates of the photoreceptors, as well as our experience with colors and expectations.\r\n\r\n<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Cone-response.svg/500px-Cone-response.svg.png\" width=\"320px\" /><br />\r\nHuman retinal cell response curves. Image from Wikimedia Commons.\r\n\r\n### RGB color space\r\n\r\nOn your computer, color is represented as three values, representing intensity of three color channels: red, green, and blue.\r\nThe pixels in the display are composed of these three basic lights.\r\nWhen all are fully on, the color is white; when all are off, the color is black.\r\nAll of the millions of colors that a modern computer is able to display come from mixing the three intensities.\r\n\r\nThe RGB system can be thought of as describing a three dimensional space, with the Red, Green, and Blue dimensions.\r\nA point in that space, given by the three coordinates, is a color.\r\nWe can begin to think of distances between colors in this way, as a distance in 3D space between two points.\r\n\r\n<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/83/RGB_Cube_Show_lowgamma_cutout_b.png/320px-RGB_Cube_Show_lowgamma_cutout_b.png\" width=\"320px\" /><br />\r\nRGB Color Cube. Image from Wikimedia Commons.\r\n\r\n### HSV color space\r\n\r\nAn additive mixture of three distinct colors does not match our intutive model of how color works: we can't easily visualize the effect of adding red, green, or blue to a color.\r\n\r\nAdditionally, the distances in RGB spaces do not match up to perceptual judgements.\r\nThat is, a color may be quite far away from another one in terms of RGB coordinates, but humans will judge the two colors as quite similar.\r\nOr the other way: two colors may *look* very different but be close together in RGB space.\r\n\r\nThe rainbow is what we usually visualize when we think of color: hues of the visible spectrum from the almost-infrared to the almost-ultraviolet, roughly divided into less than a dozen words.\r\nA given hue can be imagined as more vibrant than baseline---deep red, midnight blue---or as more pastel-like---pink lipstick, robin's egg blue.\r\n\r\nThe Hue-Saturation-Value color space is informed by this mental model, and strives to have one dimension corresponding to our intuitive notion of hue, and two dimensions which set the vibrancy and lightness.\r\nA point in HSV space is therefore more easily interpretable than a point in RGB space.\r\n\r\n<img src=\"http://upload.wikimedia.org/wikipedia/commons/f/f1/HSV_cone.jpg\" width=\"320px\" /><br />\r\nHSV Color Cone. Image from Wikimedia Commons.\r\n\r\n### Perceptually uniform color spaces\r\n\r\nAlthough it better matches our mental model of color, HSV space still suffers from a misalignment of perceptual judgements to 3D distance of colors.\r\n\r\nThe international standards agency has formulated an alternative color space with the explicit goal of making distances in the color space correspond to human judgments of color similarity.\r\nActually, it couldn't decide between two: CIELab and CIELuv.\r\n\r\nCIELab, roughly, is formed by an oval with two axes: a and b, which correspond to the \"opponent\" colors of Yellow-Blue and Red-Green.\r\nThe third dimension of L\\*a*b\\* space is lightness, which is approximately self-describing.\r\n\r\nAs an aside, the \"opponent\" colors are so named because of the [opponent process](http://en.wikipedia.org/wiki/Opponent_process) theory, which posits that color perception comes not from the asbolute values but from the *difference* in activation rates of the three types of cones in the retina.\r\nThe opponent colors have no in-between point: we can imagine a point between blue and red, but not between blue and yellow; between red and yellow, but not between red and green; and so on.\r\n\r\nIn the L\\*a*b\\* space, simple Euclidean distance (\\\\(\\sqrt{L^2 + a^2 + b^2}\\\\) between two colors, which corresponds to the intuitive notion of a distance between 3D points, is a good approximation to perceptual judgements of their difference.\r\n\r\n<img src=\"http://www.couleur.org/spaces/Labspace.jpg\" width=\"320px\" /><br />\r\nCIELab color space. Image from couleur.org.\r\n\r\n## Representing Multiple Colors\r\n\r\nAt this point, we understand that we should use the CIELab space to represent colors as 3D points, and can treat distances between them as corresponding to perceptual judgements.\r\n\r\nBut how do we represent a whole image, composed of many colors?\r\n\r\nOur solution is to introduce a canonical *palette* of colors: a small set of colors that approximately cover the color space.\r\nThe color content of any image can then be represented as a histogram over the palette colors.\r\nTo construct the histogram, for each color in the palette we find the percentage of pixels in the image that are *nearest* (in terms of Euclidean distance) to that color.\r\n\r\nWe can represent this information in a slightly different way, by showing the top colors present in the image in a type of \"palette image\", with the area of a color in the palette image proportional to the prevalence of that color in the image.\r\n\r\n<img src=\"doc/images/examples1.png\" width=\"800px\" /><br />\r\nExample of four images, their raw color histograms, and their \"palette images.\"\r\n\r\n### Similarity of color content\r\n\r\nAs described, we can represent the color content of an image with a histogram over a fixed palette, and note that images with similar color content should have similar histograms.\r\n\r\nA natural measure of similarity between histograms is the amount of overlap.\r\nImagine superimposing one histogram on another; the areas that intersect form the overlap.\r\n\r\nNow look at the third image above, of a sunset over dark hills.\r\nThe entirety of the sky seems to be in mostly two colors: a specific shade of yellow and a specific shade of light orange.\r\n\r\nBut what if another image of a sunset, very similar to us perceptually, has two colors that are just slightly different?\r\nThe histogram may in fact have no overlap at all then.\r\n\r\n<img src=\"doc/images/smoothing.png\" width=\"640px\" /><br />\r\nAn example of two images that are very similar perceptually having different raw histograms.\r\n\r\n### Smoothing histograms\r\n\r\nThe solution, as shown in the figure above, is to *smooth* the histograms---intuitively, spread the content of bins to nearby bins.\r\nNote that the smoothed histograms look practically the same for the two images, even though the unsmoothed histograms have almost no overlap.\r\n\r\nTherefore, our searchable representation of images will be in smoothed histograms.\r\n\r\n<img src=\"doc/images/examples1.png\" width=\"800px\" /><br />\r\n<img src=\"doc/images/examples2.png\" width=\"800px\" /><br />\r\nExample of four images, their raw color histograms, their \"palette images.\", and their smoothed histograms.\r\n\r\nWe smooth histograms with a Gaussian kernel, with a parameter \\\\(\\sigma\\\\) controlling the amount of smoothing.\r\n\r\n<img src=\"doc/images/exponential_smoothing.png\" width=\"320px\" /><br />\r\nEffect of sigma on the amount of smoothing.\r\n\r\nRayleigh offers a choice of a couple of settings of the sigma parameter.\r\n\r\n### Histogram distance metrics\r\n\r\nHow should we measure the distance between two color histograms?\r\n\r\nLet's say our canonical color palette has \\\\(K\\\\) colors.\r\nThe histogram over those colors can be thought of as a point (or vector) in \\\\(K\\\\)-dimensional space, with the value of dimension \\\\(k\\\\) corresponding to the amount of the \\\\(k\\\\)'th color of the palette in the image histogram.\r\n\r\n**Euclidean** or \\\\(L_2\\\\) distance between two histograms \\\\(H\\\\) and \\\\(H'\\\\) is simply \\\\(\\sqrt{\\sum_{i=1}^K (H_i-H'_i)^2}\\\\) and corresponds to the intuitive notion of distance between the two vectors in \\\\(K\\\\)-dimensional space.\r\nThe calculation of this distance is quite fast due to its possible implementation as simple dot product, for which there are very fast system-level libraries.\r\n\r\nA similar metric is **Manhattan** or \\\\(L_1\\\\) distance, which is simply \\\\(\\sum_{i=1}^K \\left|H_i-H'_i\\right|\\\\).\r\nA better distance metric for histograms is **Chi-squared** distance: \\\\(\\sum_{i=1}^K \\frac{(H_i - H'_i)^2}{H_i + H'_i}\\\\), or the **histogram intersection** distance: \\\\(\\sum_{i=1}^K \\min(H_i, H'_i)\\\\).\r\n\r\nRayleigh offers a choice between the first three distances.\r\n\r\n## Forming the Palette\r\n\r\nRecall that before we could represent an image as a histogram, we had to decide on the canonical colors of the histogram.\r\nThe palette should reasonably cover the space of all possible colors, and, as it will be used as part of the search-by-palette UI, should appear intuitively \"correct.\"\r\n\r\nRepresenting the 3D space of colors in two dimensions is a problem that has no fully correct solution.\r\nMy solution was to use the HSV color space and create a grid of colors by\r\n\r\n1. picking N hues such that they span the 360 degrees of the color wheel, and presenting them as the middle row of the grid with full saturation and value.\r\n2. for each hue, offering\r\n    1. K variations by decreasing saturation\r\n    2. K variations by decreasing lightness\r\n    3. An additional K-1 variations by decreasing both saturation and lightness.\r\n3. Add as many grayscale levels as there are variations, as the last column.\r\n\r\n<img src=\"doc/images/palette_8_2.png\" width=\"200px\" /><br />\r\nPalette of 8 hues and 2 variations.\r\n\r\n<img src=\"doc/images/palette_14_3.png\" width=\"320px\" /><br />\r\nPalette of 14 hues and 3 variations.\r\n\r\n<!--\r\nfrom skimage.io import imread, imshow, imsave\r\nim = imread('doc/images/palette_14_3.png')\r\nimsave('temp.png', scipy.misc.imresize(im, np.array(im.shape[:2])*50, interp='nearest'))\r\n-->\r\n\r\n## Constructing the dataset\r\n\r\nAll the components are now in place to construct a system that allows searching a large number of images by multiple colors, or by color similarity to another image.\r\n\r\nWe use the Flickr API to construct a large set of images.\r\nA [particular API call](http://www.flickr.com/services/api/flickr.interestingness.getList.html) returns up to 500 most \"interesting\" images, as judged by an internal Flickr algorithm that is based on user actions such as Favorites, comments, and addition to sets for a given date.\r\nWe assemble a set of a million images using such calls (code is provided in this repository).\r\n\r\n## Searching\r\n\r\nDownloading and processing a million images is not trivial.\r\nThe images are loaded in parallel into a MongoDB database.\r\nFor each image, a raw histogram over the canonical color palette is computed and stored alongside image dimensions, url, and Flickr ID.\r\nThe pixel data is not stored, as we do not seek to replicate Flickr's data but point to it.\r\n\r\nA searchable colection is formed from this data by loading all the raw histograms into a big matrix and smoothing them with a given parameter sigma.\r\nThe matrix forms the basis for the *Exact* search mode, where distances from the query to all images in the dataset are computed and the top K results are returned.\r\n\r\nRayleigh's color palette consists of \\\\(D=88\\\\) colors, and the current production version has \\\\(N=100K\\\\) images loaded.\r\nThis matrix easily fits in memory of even a basic computer, and so we can ignore disk seek effects and correctly estimate the cost of exact search as \\\\(O(DN)\\\\), as we have to look at all histograms and compute at least a dot product with each one.\r\n\r\nThere are two ways to speed this up:\r\n\r\n1. Reduce the dimensionality of the data \\\\(D\\\\)\r\n2. Provide an efficient index to the data; accept potentially inexact results.\r\n\r\n### Dimensionality reduction\r\n\r\nThis speed-up does not yield us much, as the dimensionality of our data is already quite low.\r\nNevertheless, Rayleigh implements PCA-based dimensionality reduction that works quite well if you'd like to experiment with larger palettes.\r\nAn interesting question that I haven't looked at is what the PCA-reduced L\\*a\\*b\\* colors look like.\r\n\r\n### Indexing\r\n\r\nA better approach is to construct an index to the data, such that we can quickly find all data points that have low distance to the query data point, without having to look through the entire dataset.\r\n\r\nIn addition to the Exact method above, Rayleigh implements SearchableImageCollection's using two packages.\r\n\r\n[**cKDTree**](http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.html) implements a standard data structure for such indexing tasks: the KD Tree, which splits the data in two, one dimension at a time, such that a group of nearest neighbors can quickly be found for a query.\r\n\r\n[**Fast Library for Approximate Nearest Neighbors**](http://www.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN) is solidly-developed research code that is used and partially developed by the impressive [Point Cloud Library](http://www.pointclouds.org) project.\r\n\r\nThe basic idea is automatic parameter tuning and comparison of two different methods for indexing data: multiple KD trees and hierarchical k-means.\r\nParameters are tuned such that the index is fast but can return inexact results.\r\n\r\nWe found that FLANN performs very well; since it includes kd trees, we did not extensively experiment with the cKDTree.\r\n\r\n## System Architecture & Results\r\n\r\nThe separation of concerns of our application are:\r\n\r\n- ImageCollection and SearchableImageCollection process images and construct index.\r\n\r\n- Flask web app provides a REST interface to the SearchableImageCollection and some utility methods, such as plotting color histograms.\r\n\r\n- HTML/JS UI is output by Flask, which fills in some things on the server side, but the search results are fetched with a separate AJAX call, to provide easy extension to loading multiple pages of results in a scrolling interface.\r\n\r\nPlease use the [demo](http://ec2-204-236-191-162.us-west-1.compute.amazonaws.com) and play around with different settings and color queries.\r\n\r\nI would appreciate suggestions on how to effectively display a summary of results here.\r\n\r\n## Next Steps\r\n\r\n<!--\r\n    For ease of reading, you might want to integrate a brief conclusion (one bullet point would suffice) into your next steps section (e.g. Conclusion and Next Steps). But I like things summarized and redundant so take that with a grain of salt... \r\n\r\n    Jessica\r\n-->\r\n\r\n- In constructing a histogram, weigh pixels by visual saliency. Often, the colors we perceive as important in the image actually do not take up much area and so are not well represented in the image histogram.\r\n\r\n- Provide UI for adjusting percentages of each color in the palette query. No backend work is needed.\r\n\r\n- How to represent \"do-not-care\" colors?\r\n\r\n- Threshold-and-re-normalize the color histograms: cut off normalized histogram mass above some threshold, and re-normalize. This is a trick used in constructing the [SIFT descriptor](http://en.wikipedia.org/wiki/Scale-invariant_feature_transform#Keypoint_descriptor), for example.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}